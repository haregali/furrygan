{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorNetwork(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, ngf):\n",
    "        self.input_nc = input_nc\n",
    "        self.output_nc = output_nc\n",
    "        self.ngf = 64\n",
    "        self.norm_type = 'batch'\n",
    "        self.n_downsample = 2\n",
    "        self.n_blocks_global = 9\n",
    "        self.n_local_enhancers = 1\n",
    "        self.n_blocks_local = 3\n",
    "        self.embed_nc = 256*5\n",
    "        self.padding_type='reflect'\n",
    "\n",
    "        super(EmbedGlobalBGGenerator, self).__init__()\n",
    "        norm_layer = get_norm_layer(norm_type=self.norm_type)\n",
    "        activation = nn.ReLU(True)\n",
    "        \n",
    "        downsample_model = [nn.ReflectionPad2d(3), nn.Conv2d(self.input_nc, self.ngf, kernel_size=7, padding=0), norm_layer(self.ngf), activation]\n",
    "        \n",
    "        for i in range(self.n_downsample):\n",
    "            mult = 2**i\n",
    "            if i != self.n_downsample-1:\n",
    "                downsample_model += [nn.Conv2d(self.ngf * mult, self.ngf * mult * 2, kernel_size=3, stride=2, padding=1),\n",
    "                      norm_layer(self.ngf * mult * 2), activation]\n",
    "            else:\n",
    "                downsample_model += [nn.Conv2d(self.ngf * mult, self.ngf * mult * 2, kernel_size=3, stride=2, padding=1),\n",
    "                      norm_layer(self.ngf * mult * 2), activation]\n",
    "        self.downsample_model = nn.Sequential(*downsample_model)\n",
    "        \n",
    "        model=[]\n",
    "        model += [nn.Conv2d(in_channels=self.ngf*(2**self.n_downsample)+self.embed_nc, out_channels=self.ngf*(2**self.n_downsample), kernel_size=1, padding=0, stride=1, bias=True)]\n",
    "\n",
    "        mult = 2**self.n_downsample\n",
    "        for i in range(self.n_blocks_global):\n",
    "            self.padding_type='reflect'\n",
    "            model += [ResnetBlock(self.ngf * mult, padding_type=self.padding_type, activation=activation, norm_layer=norm_layer)]\n",
    "        \n",
    "        ### upsample         \n",
    "        for i in range(self.n_downsample):\n",
    "            mult = 2**(self.n_downsample - i)\n",
    "            model += [nn.ConvTranspose2d(self.ngf * mult, int(self.ngf * mult / 2), kernel_size=4, stride=2, padding=1, output_padding=0),\n",
    "                       norm_layer(int(self.ngf * mult / 2)), activation]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "        bg_encoder = [nn.ReflectionPad2d(3), nn.Conv2d(3, self.ngf, kernel_size=7, padding=0), norm_layer(self.ngf), activation]\n",
    "        self.bg_encoder = nn.Sequential(*bg_encoder)\n",
    "\n",
    "        bg_decoder = [nn.Conv2d(in_channels=ngf*2, out_channels=self.ngf, kernel_size=1, padding=0, stride=1, bias=True)]\n",
    "        bg_decoder += [nn.ReflectionPad2d(3), nn.Conv2d(self.ngf, output_nc, kernel_size=7, padding=0), nn.Tanh()]\n",
    "        self.bg_decoder = nn.Sequential(*bg_decoder)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "\n",
    "    \n",
    "class DiscriminatorNetwork(nn.Module):\n",
    "    def __init__(self, input_nc, dis_n_layers, use_sigmoid):\n",
    "        self.input_nc = input_nc\n",
    "        self.dis_n_layers = dis_n_layers\n",
    "        self.use_sigmoid = use_sigmoid\n",
    "        self.ndf = 64\n",
    "        self.norm_type = 'batch'\n",
    "        self.num_D = 2\n",
    "        self.getIntermFeat = True\n",
    "        \n",
    "        super(DiscriminatorNetwork, self).__init__()\n",
    "        \n",
    "        norm_layer = get_norm_layer(norm_type=self.norm_type)\n",
    " \n",
    "        for i in range(self.num_D):\n",
    "            netD = NLayerDiscriminator(self.input_nc, self.ndf, self.dis_n_layers, norm_layer, use_sigmoid, getIntermFeat)\n",
    "            if getIntermFeat:                                \n",
    "                for j in range(n_layers+2):\n",
    "                    setattr(self, 'scale'+str(i)+'_layer'+str(j), getattr(netD, 'model'+str(j)))                                   \n",
    "            else:\n",
    "                setattr(self, 'layer'+str(i), netD.model)\n",
    "\n",
    "        self.downsample = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\n",
    "        \n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "def get_norm_layer(norm_type='instance'):\n",
    "    if norm_type == 'batch':\n",
    "        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n",
    "    elif norm_type == 'instance':\n",
    "        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False)\n",
    "    else:\n",
    "        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n",
    "    return norm_layer\n",
    "\n",
    "class DecoderGenerator_mask_skin_image(nn.Module):\n",
    "    def __init__(self, norm_layer):  \n",
    "        super(DecoderGenerator_mask_skin_image, self).__init__()\n",
    "        self.fc = nn.Sequential(nn.Linear(in_features=512, out_features=512*2*2))\n",
    "        # input is 512*2*2\n",
    "        layers_list = []\n",
    "        layers_list.append(DecoderBlock(channel_in=512, channel_out=512, kernel_size=4, padding=1, stride=2, output_padding=0))  #128*4\n",
    "        layers_list.append(DecoderBlock(channel_in=512, channel_out=512, kernel_size=4, padding=1, stride=2, output_padding=0))  #128*8*8\n",
    "        layers_list.append(DecoderBlock(channel_in=512, channel_out=512, kernel_size=4, padding=1, stride=2, output_padding=0))  #128*16*16\n",
    "        layers_list.append(DecoderBlock(channel_in=512, channel_out=512, kernel_size=4, padding=1, stride=2, output_padding=0))  #128*32*32\n",
    "        layers_list.append(DecoderBlock(channel_in=512, channel_out=256, kernel_size=4, padding=1, stride=2, output_padding=0))  #128*64*64\n",
    "        layers_list.append(DecoderBlock(channel_in=256, channel_out=128, kernel_size=4, padding=1, stride=2, output_padding=0))  #64*128*128\n",
    "        layers_list.append(DecoderBlock(channel_in=128, channel_out=64, kernel_size=4, padding=1, stride=2, output_padding=0))  #64*256*256\n",
    "        layers_list.append(nn.ReflectionPad2d(2))\n",
    "        layers_list.append(nn.Conv2d(64,3,kernel_size=5,padding=0))\n",
    "        layers_list.append(nn.Tanh())\n",
    "        \n",
    "        self.conv = nn.Sequential(*layers_list)\n",
    "\n",
    "    def forward(self, ten):\n",
    "        # print(\"in DecoderGenerator_mask_skin, print some shape \")\n",
    "        ten = self.fc(ten)\n",
    "        ten = ten.view(ten.size()[0],512, 2, 2)\n",
    "        ten = self.conv(ten)\n",
    "        assert ten.size()[1] == 3\n",
    "        assert ten.size()[2] == 256\n",
    "        assert ten.size()[3] == 256\n",
    "        return ten\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return super(DecoderGenerator_mask_skin_image, self).__call__(*args, **kwargs)\n",
    "class DecoderGenerator_mask_mouth_image(nn.Module):\n",
    "    def __init__(self, norm_layer):  \n",
    "        super(DecoderGenerator_mask_mouth_image, self).__init__()\n",
    "        # start from B*1024\n",
    "        # self.fc = nn.Sequential(nn.Linear(in_features=1024, out_features=512*4*4),\n",
    "        #                         nn.BatchNorm1d(num_features=512*4*4, momentum=0.9),\n",
    "        #                         nn.ReLU(True))\n",
    "        self.fc = nn.Sequential(nn.Linear(in_features=512, out_features=512*5*9))\n",
    "        layers_list = []\n",
    "        # layers_list.append(nn.BatchNorm2d(256, momentum=0.9))\n",
    "        # layers_list.append(nn.ReLU(True))\n",
    "\n",
    "        layers_list.append(DecoderBlock(channel_in=512, channel_out=256, kernel_size=4, padding=1, stride=2, output_padding=0)) #10*18\n",
    "        layers_list.append(DecoderBlock(channel_in=256, channel_out=128, kernel_size=4, padding=1, stride=2, output_padding=0)) #20*36\n",
    "        layers_list.append(DecoderBlock(channel_in=128, channel_out=64, kernel_size=4, padding=1, stride=2, output_padding=0)) #40*72\n",
    "        layers_list.append(DecoderBlock(channel_in=64, channel_out=64, kernel_size=4, padding=1, stride=2, output_padding=0)) #80*144\n",
    "        # layers_list.append(DecoderBlock(channel_in=64, channel_out=64, kernel_size=4, padding=1, stride=2, output_padding=0)) #96*160\n",
    "        layers_list.append(nn.ReflectionPad2d(2))\n",
    "        layers_list.append(nn.Conv2d(64,3,kernel_size=5,padding=0))\n",
    "        layers_list.append(nn.Tanh())\n",
    "\n",
    "        # layers_list.append(DecoderBlock(channel_in=256, channel_out=256, kernel_size=3, padding=1, stride=1, output_padding=0)) #256*12*14\n",
    "\n",
    "        self.conv = nn.Sequential(*layers_list)\n",
    "\n",
    "    def forward(self, ten):\n",
    "        # print(\"in DecoderGenerator, print some shape \")\n",
    "        ten = self.fc(ten)\n",
    "        ten = ten.view(ten.size()[0],512, 5, 9)\n",
    "        ten = self.conv(ten)\n",
    "        assert ten.size()[1] == 3\n",
    "        assert ten.size()[2] == 80\n",
    "        assert ten.size()[3] == 144\n",
    "        return ten\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return super(DecoderGenerator_mask_mouth_image, self).__call__(*args, **kwargs)\n",
    "\n",
    "\n",
    "class DecoderGenerator_mask_eye_image(nn.Module):\n",
    "    def __init__(self, norm_layer):  \n",
    "        super(DecoderGenerator_mask_eye_image, self).__init__()\n",
    "        # start from B*1024\n",
    "        # self.fc = nn.Sequential(nn.Linear(in_features=1024, out_features=512*4*4),\n",
    "        #                         nn.BatchNorm1d(num_features=512*4*4, momentum=0.9),\n",
    "        #                         nn.ReLU(True))\n",
    "        self.fc = nn.Sequential(nn.Linear(in_features=512, out_features=512*2*3, bias=False))\n",
    "        layers_list = []\n",
    "        # layers_list.append(nn.BatchNorm2d(256, momentum=0.9))\n",
    "        # layers_list.append(nn.ReLU(True))\n",
    "\n",
    "        layers_list.append(DecoderBlock(channel_in=512, channel_out=256, kernel_size=4, padding=1, stride=2, output_padding=0)) #256*4\n",
    "        layers_list.append(DecoderBlock(channel_in=256, channel_out=128, kernel_size=4, padding=1, stride=2, output_padding=0)) #128*8\n",
    "        layers_list.append(DecoderBlock(channel_in=128, channel_out=64, kernel_size=4, padding=1, stride=2, output_padding=0)) #64*16\n",
    "        layers_list.append(DecoderBlock(channel_in=64, channel_out=64, kernel_size=4, padding=1, stride=2, output_padding=0)) #64*32\n",
    "        # layers_list.append(DecoderBlock(channel_in=64, channel_out=64, kernel_size=4, padding=1, stride=2, output_padding=0)) #64*64\n",
    "        layers_list.append(nn.ReflectionPad2d(2))\n",
    "        layers_list.append(nn.Conv2d(64,3,kernel_size=5,padding=0))\n",
    "        layers_list.append(nn.Tanh())\n",
    "\n",
    "        # layers_list.append(DecoderBlock(channel_in=256, channel_out=256, kernel_size=3, padding=1, stride=1, output_padding=0)) #256*12*14\n",
    "\n",
    "        self.conv = nn.Sequential(*layers_list)\n",
    "\n",
    "    def forward(self, ten):\n",
    "        # print(\"in DecoderGenerator, print some shape \")\n",
    "        ten = self.fc(ten)\n",
    "        ten = ten.view(ten.size()[0],512, 2, 3)\n",
    "        ten = self.conv(ten)\n",
    "        assert ten.size()[1] == 3\n",
    "        assert ten.size()[2] == 32\n",
    "        assert ten.size()[3] == 48\n",
    "        return ten\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return super(DecoderGenerator_mask_eye_image, self).__call__(*args, **kwargs)\n",
    "\n",
    "\n",
    "class DecoderGenerator_mask_mouth(nn.Module):\n",
    "    def __init__(self, norm_layer):  \n",
    "        super(DecoderGenerator_mask_mouth, self).__init__()\n",
    "        \n",
    "\n",
    "        self.fc = nn.Sequential(nn.Linear(in_features=512, out_features=512*5*9))\n",
    "        layers_list = []\n",
    "\n",
    "        layers_list.append(DecoderBlock(channel_in=512, channel_out=256, kernel_size=4, padding=1, stride=2)) #10*18\n",
    "        layers_list.append(DecoderBlock(channel_in=256, channel_out=256, kernel_size=4, padding=1, stride=2)) #20*36\n",
    "        # layers_list.append(DecoderBlock(channel_in=256, channel_out=256, kernel_size=4, padding=1, stride=2)) #40*72\n",
    "\n",
    "        self.conv = nn.Sequential(*layers_list)\n",
    "\n",
    "    def forward(self, ten):\n",
    "        # print(\"in DecoderGenerator, print some shape \")\n",
    "        # ten = self.fc(ten)\n",
    "        # ten = ten.view(ten.size()[0],512, 4, 4)\n",
    "        ten = self.fc(ten)\n",
    "        ten = ten.view(ten.size()[0],512, 5, 9)\n",
    "        ten = self.conv(ten)\n",
    "        assert ten.size()[1] == 256\n",
    "        assert ten.size()[2] == 20\n",
    "        assert ten.size()[3] == 36\n",
    "        return ten\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return super(DecoderGenerator_mask_mouth, self).__call__(*args, **kwargs)\n",
    "\n",
    "\n",
    "class DecoderGenerator_mask_eye(nn.Module):\n",
    "    def __init__(self, norm_layer):  \n",
    "        super(DecoderGenerator_mask_eye, self).__init__()\n",
    "        # start from B*1024\n",
    "        # self.fc = nn.Sequential(nn.Linear(in_features=1024, out_features=512*4*4),\n",
    "        #                         nn.BatchNorm1d(num_features=512*4*4, momentum=0.9),\n",
    "        #                         nn.ReLU(True))\n",
    "        # self.fc = nn.Sequential(nn.Linear(in_features=1024, out_features=256*6*7, bias=False))\n",
    "        self.fc = nn.Sequential(nn.Linear(in_features=512, out_features=512*2*3, bias=False))\n",
    "        layers_list = []\n",
    "        # layers_list.append(nn.BatchNorm2d(256, momentum=0.9))\n",
    "        # layers_list.append(nn.ReLU(True))\n",
    "        layers_list.append(DecoderBlock(channel_in=512, channel_out=256, kernel_size=4, padding=1, stride=2)) #256*4\n",
    "        layers_list.append(DecoderBlock(channel_in=256, channel_out=256, kernel_size=4, padding=1, stride=2)) #256*8\n",
    "        # layers_list.append(DecoderBlock(channel_in=256, channel_out=256, kernel_size=4, padding=1, stride=2)) #256*16\n",
    "        # layers_list.append(DecoderBlock(channel_in=256, channel_out=256, kernel_size=3, padding=1, stride=1)) #256*16\n",
    "        # # layers_list.append(DecoderBlock(channel_in=256, channel_out=256, kernel_size=3, padding=1, stride=1, output_padding=0)) #256*12*14\n",
    "\n",
    "        self.conv = nn.Sequential(*layers_list)\n",
    "\n",
    "    def forward(self, ten):\n",
    "        # print(\"in DecoderGenerator, print some shape \")\n",
    "        # ten = self.fc(ten)\n",
    "        # ten = ten.view(ten.size()[0],512, 4, 4)\n",
    "        ten = self.fc(ten)\n",
    "        ten = ten.view(ten.size()[0],512, 2, 3)\n",
    "        ten = self.conv(ten)\n",
    "        assert ten.size()[1] == 256\n",
    "        assert ten.size()[2] == 8\n",
    "        assert ten.size()[3] == 12\n",
    "        return ten\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return super(DecoderGenerator_mask_eye, self).__call__(*args, **kwargs)\n",
    "\n",
    "\n",
    "class DecoderGenerator_mask_skin(nn.Module):\n",
    "    def __init__(self, norm_layer):  \n",
    "        super(DecoderGenerator_mask_skin, self).__init__()\n",
    "        # input is 128*4*4\n",
    "        self.fc = nn.Sequential(nn.Linear(in_features=512, out_features=512*2*2))\n",
    "        layers_list = []\n",
    "        layers_list.append(DecoderBlock(channel_in=512, channel_out=256, kernel_size=4, padding=1, stride=2))  #256*4\n",
    "        layers_list.append(DecoderBlock(channel_in=256, channel_out=256, kernel_size=4, padding=1, stride=2))  #256*8\n",
    "        layers_list.append(DecoderBlock(channel_in=256, channel_out=256, kernel_size=4, padding=1, stride=2))  #256*16\n",
    "        layers_list.append(DecoderBlock(channel_in=256, channel_out=256, kernel_size=4, padding=1, stride=2))  #256*32\n",
    "        layers_list.append(DecoderBlock(channel_in=256, channel_out=256, kernel_size=4, padding=1, stride=2))  #256*64\n",
    "        self.conv = nn.Sequential(*layers_list)\n",
    "\n",
    "    def forward(self, ten):\n",
    "        # print(\"in DecoderGenerator_mask_skin, print some shape \")\n",
    "        ten = self.fc(ten)\n",
    "        ten = ten.view(ten.size()[0],512, 2, 2)\n",
    "        ten = self.conv(ten)\n",
    "        assert ten.size()[1] == 256\n",
    "        assert ten.size()[2] == 64\n",
    "        return ten\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return super(DecoderGenerator_mask_skin, self).__call__(*args, **kwargs)\n",
    "\n",
    "class FurryGan(nn.Module):\n",
    "    def __init__(self, isTrain=True):\n",
    "        self.input_nc = 11             #number of input channels\n",
    "        self.output_nc = 3             #number of output channels\n",
    "        self.isTrain = isTrain         #Whether to train\n",
    "        self.dis_net_input_nc = self.input_nc + self.output_nc\n",
    "        self.dis_n_layers = 3\n",
    "        \n",
    "        self.gen_net = GeneratorNetwork(self.input_nc, self.output_nc)\n",
    "        self.gen_net.apply(weights_init)\n",
    "        \n",
    "        if self.isTrain:\n",
    "            use_sigmoid = True\n",
    "            \n",
    "        \n",
    "        self.dis_net = DiscriminatorNetwork(self.dis_net_input_nc, self.dis_n_layers, use_sigmoid)\n",
    "        \n",
    "        #TODO\n",
    "#         embed_feature_size\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.decoder_skin_net = DecoderGenerator_mask_skin(functools.partial(nn.BatchNorm2d, affine=True))\n",
    "        self.decoder_skin_hair = DecoderGenerator_mask_skin(functools.partial(nn.BatchNorm2d, affine=True))\n",
    "        self.decoder_skin_left_eye =  DecoderGenerator_mask_eye(functools.partial(nn.BatchNorm2d, affine=True))\n",
    "        self.decoder_skin_right_eye = DecoderGenerator_mask_eye(functools.partial(nn.BatchNorm2d, affine=True))\n",
    "        self.decoder_skin_mouth =  DecoderGenerator_mask_mouth(functools.partial(nn.BatchNorm2d, affine=True)) \n",
    "        \n",
    "        \n",
    "        self.decoder_skin_image_net = DecoderGenerator_mask_skin_image(functools.partial(nn.BatchNorm2d, affine=True))\n",
    "        self.decoder_skin_image_hair = DecoderGenerator_mask_skin_image(functools.partial(nn.BatchNorm2d, affine=True))\n",
    "        self.decoder_skin_image_left_eye = DecoderGenerator_mask_eye_image(functools.partial(nn.BatchNorm2d, affine=True))\n",
    "        self.decoder_skin_image_right_eye = DecoderGenerator_mask_eye_image(functools.partial(nn.BatchNorm2d, affine=True))\n",
    "        self.decoder_skin_image_mouth = DecoderGenerator_mask_mouth_image(functools.partial(nn.BatchNorm2d, affine=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
